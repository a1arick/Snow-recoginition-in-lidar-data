{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8373b8",
   "metadata": {},
   "source": [
    "# В этом ноутбуке тренируется и тестируется модель PointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/alexander.savelyev/PycharmProjects/Snow_recognition')\n",
    "from PointNet import PointNetDenseCls\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "EQUAL_ASPECT_RATIO_LAYOUT = dict(\n",
    "    margin={\n",
    "        'l': 0,\n",
    "        'r': 0,\n",
    "        'b': 0,\n",
    "        't': 0\n",
    "    }, scene=dict(\n",
    "    aspectmode='data'\n",
    "))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e95b91",
   "metadata": {},
   "source": [
    "Мы будем использовать оригинальный набор данных, потому что архитектура своими нейросетевыми кодировщиками сама будет решать как нормализовать данные. А также эта модель задумывалась на применение ее сразу после получения результатов работы лидара, чтобы не тратить время на препроцессинг "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b17c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snow3D(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, path: str, train: bool, features=['x', 'y', 'z']):\n",
    "        file = '/Volumes/HP P800/itmo/Lidar data/data_train.csv' if train else '/Volumes/HP P800/itmo/Lidar data/data_test.csv'\n",
    "        self.data = pd.read_csv(file, index_col='scene_id')\n",
    "        self.features = features\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.index.unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError\n",
    "        queried_data = self.data.loc[idx]\n",
    "        X = torch.tensor(queried_data[self.features].to_numpy()).type(torch.FloatTensor)\n",
    "        if self.train:\n",
    "            y = torch.tensor(queried_data['label'].to_numpy()).type(torch.LongTensor)\n",
    "            return X, y\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a8470",
   "metadata": {},
   "source": [
    "Генерируем тренировочный, валидационный и тестовый датасеты. Валидационный датасет нужен, чтобы модель не переобучилась на тренировочных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Snow3D(dataset_path, train=True)\n",
    "train_size = int(0.9 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(\n",
    "    train_data,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "test_data = Snow3D(dataset_path, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46920e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7255b3f",
   "metadata": {},
   "source": [
    "Определяем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4003196",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointNetDenseCls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b1edc",
   "metadata": {},
   "source": [
    "Посмотрим на ее архитектуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128275f2",
   "metadata": {},
   "source": [
    "Определяем функцию потерь, и оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b93884",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9861ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 50\n",
    "train_ts, train_loss = [], []\n",
    "val_ts, val_loss, val_acc = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для отслживания прогресса при обучении\n",
    "def show_progress(t):\n",
    "    clear_output(wait=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(20, 5))\n",
    "    fig.suptitle(f'Epoch {t:3.3f}', fontsize=16)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.set_xlabel('time (epochs)')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.plot(train_ts, train_loss, c='darkblue', lw=3)\n",
    "    ax1.plot(val_ts, val_loss, c='green', marker='o', lw=5)\n",
    "    ax2.set_title('accuracy')\n",
    "    ax2.set_xlabel('time (epochs)')\n",
    "    ax2.plot(val_ts, val_acc, c='green', marker='o', lw=5)\n",
    "    plt.show()\n",
    "\n",
    "# функция для тренировки\n",
    "def train(epoch, dataset, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    n_scenes = len(dataset)\n",
    "    for scene, (X, y) in enumerate(dataset):\n",
    "        # Send data to training device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "\n",
    "        #print(X.shape)\n",
    "        X = torch.unsqueeze(X, 0).swapaxes(1,2)\n",
    "        #print(g.shape)\n",
    "        \n",
    "        pred = model(X)\n",
    "        #print(pred[0].shape)\n",
    "        print(pred[0][0,:,1].shape)\n",
    "        print(y.type(torch.FloatTensor).shape)\n",
    "        loss = loss_fn(pred[0][0,:,1].unsqueeze(0), y.type(torch.FloatTensor).unsqueeze(0))\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Progress output\n",
    "        if scene % interval == 0:\n",
    "            t = epoch + (scene + 1) / n_scenes\n",
    "            train_ts.append(t)\n",
    "            train_loss.append(loss.item())\n",
    "            show_progress(t)\n",
    "\n",
    "# функция для тестирования\n",
    "def test(epoch, dataset, model, loss_fn):\n",
    "    model.eval()\n",
    "    n_scenes = len(dataset)\n",
    "    n_points = 0\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataset:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            n_points += len(y)\n",
    "    test_loss /= n_scenes\n",
    "    correct /= n_points\n",
    "    val_ts.append(epoch + 1)\n",
    "    val_loss.append(test_loss)\n",
    "    val_acc.append(correct)\n",
    "    show_progress(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    train(t, train_data, model, loss_fn, optimizer)\n",
    "    test(t, val_data, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a39b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X in test_data:\n",
    "        X = X.to(device)\n",
    "        pred = model(X).argmax(1).cpu().numpy()\n",
    "        predictions.extend(list(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e22374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/Volumes/HP P800/itmo/Lidar data/data_test.csv', index_col='scene_id')\n",
    "y_test = df_test[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f667128",
   "metadata": {},
   "source": [
    "оценка результатов на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
